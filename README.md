# Spring 2023 Virtual AI Learning (VAIL) Program on Applied Deep Learning by SureStart 

| Sun | Mon | Tue | Wed| Thu | Fri | Sat|
|-   |-     | --  |-   |-    | -   | -  |
|  | [Day 01](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_01/) | [Day 02](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_02/) | [Day 03](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_03/) | [Day 04](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_04/) | Review | |
|  | [Day 05](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_05/) | [Day 06](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_06/) | [Day 07](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_07/) | [Day 08](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_08/) | Review | |
|  | [Day 09](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_09/) | [Day 10](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_10/) | [Day 11](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_11/) | [Day 12](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_12/) | Review | |
|  | [Day 13](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_13/) | [Day 14](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_14/) | Review | [Day 15](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_15/) | [Day 16](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_16/) | |
|  | [Day 17](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_17/) | [Day 18](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_18/) | [Day 19](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_19/) | [Day 20](https://github.com/MySureStart/Spring_2023_VAIL/tree/main/Day_20/) | Finale | |

## Phase 1

The first 3 weeks will be a self-paced learning phase with the mentoring and support from SureStart, as well as other experienced R&D professionals from other organizations, including top universities.

Day 1: We start the program off with an introduction to machine learning algorithms, and how a state of the art library makes it easy and fun to run on-the-shelf machine learning models for Linear Regression, Decision Trees and Random Forest.

Day 1, We start the program off with an introduction to machine learning algorithms, and how a state of the art library makes it easy and fun to run on-the-shelf machine learning models for Linear Regression, Decision Trees and Random Forest.


Day 2: Once we have looked at simpler machine learning models we come to slightly more powerful AI tools such as Deep Learning models and Tensorflow which is a library in python which helps build deep neural networks as easily as stacking blocks of neural layers.

Day 3: While there exist many libraries which make it easy to build many complex neural networks, they sheild us from the inner workings of the same and we get what is a blackbox in to their workings. Today we shall build a simple neural network from scratch and study it.

Day 4: Going one step further with neural network architectures, we explore an architecture widely used in Computer Vision tasks, Convolutional Neural Networks. They are known for their ability to compound smaller patterns into larger more humanly-recognizable ones.

Day 5: An important part of the learning process of any machine learning algorithm is its loss function or cost function. It maps an event or values of one or more variables onto a real number intuitively representing some "cost" associated with the event. We study loss functions related to Regression based algorithms on this day.

Day 6: Progressing from the previous day we come over to losses regarding Classification related algorithms on this day. They differ from regression loss functions in as regression loss functions aim at predicting quantities while classificaiton loss functions aim at predicting class and labels.

Day 7: The next topic is very connected to the previous topic. We can have a loss/cost function which tells how accurate the given machine learning model is at its prediction, but we need another alogrithm which is able to chagne the weigths of the model to lower the loss/cost function such that our model gets better at its predictions. We call such an algorithm optimizers.

Day 8: All neural networks on the basis of their perceptrons/nodes are still very linear in their function. We add intermediate nodes we pass our outputs through in order to add non-linearity and fit our model to more complex problems. Such nodes use what we call activation functions.

Day 9: With any machine learning models, what is of the utmost importance is data, and the quality of such data. Based on how and how much we train our models it is possible to make it learn not enough from the data or learn the details rather than the patterns of given data. We learn how to combat such situations on this day.

Day 10: We delve futher into how to handle Overfitting and Underfitting by learning of modifications we can make to our neural network for it. 

Day 11: In previous days and even today there are questions on the rammifications of the capabilities of Machine Learning algorithms. As they learn from what data we give them, it is very important to vet what it gets. As it is easier to remove bias from machines than from humans manually. We learn of things to be careful of with the ethics of Machine Learning.

Day 12: Another slightly advanced architecture of neural networks is an Autoencoder. Unlike previously discussed deep learning algorithms this architecture is unsupervised. Which means we do not give it labels to learn from, but simply the input image. We shall see similar network structure in coming days.

Day 13: A slight improvement to the Autoencoders gives us the Generative Adversarial networks. Where we have an encoder which is trained to produce the input data, and an decoder which discerns wether the data it gets is real or generated by the encoder. They essentially train each other to get better at their tasks.

Day 14: We have a task, Style Transfer, which is very connected to Generative Adversarial networks but differ in one simple way. We shall learn about this difference during this day and more.

## Phase 2

The last 2 weeks of the program will lead upto a Makeathon where students will apply their learnings from Phase 1 to conduct conduct group-based ideation and creation. The students will work in small teams of 4-5 participants to develop an AI-based solution to a real-world problem for the program Makeathon. Each team will be paired up with a mentor to provide guidance and support thorugh Phase 2.

The final projects created by the students will be presented on the last day of the program to a judging panel of the industry experts from partner tech organizations. Winning teams will recieve cash prizes and participants will recieve certificates of completion.
